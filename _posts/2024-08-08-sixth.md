---
title : 컴퓨터와 대화하는 방법-1
excerpt : Natural Language Programming
categories :
        - stduy
        - Deep Learning
toc : true
author_profile : false
sidebar :
    nav : "counts"
---

# Natural Language Programming(NLP,자연어처리) 란?
컴퓨터는 계산을 하는 기계이다. 우리에게 친근하게 말을 걸어주는 인공지능도 계산 혹은 처리를 하는 기계일 뿐이다. 그렇다면 사람과 컴퓨터는
어떻게 소통해야 할까? 또한 기계가 사람 말을 알아듣는 것처럼 보이게 하면 어떤 요소들이 있어야 할까? 
NLP 모델은 자연어를 입력받으면 확률을 출력하는 모델이다(마치 함수와도 같다). 예를 들어 오늘 저녁은 맛있어요 라는 문장이 입력값이라면 출력이 1(긍정)으로 나오게 된다.

# NLP모델의 학습 
NLP 모델에는 3가지 (긍정, 부정, 중립) 감정이 존재한다. NLP는 학습데이터를 처음 입력받으면 3가지 감정을 같은 확률로 출력한다. 우리가 결과값을 입력해주면 결과값의 감정의 확률을 높인다. 예를들어 긍정이 결과값이라면 긍정의 확률을 높인다.
이런식으로 모델을 계속 업데이트 하면서 학습한다. 학습이란 결국 출력이 정답에 가까워지도록 모델을 업데이트 하는 과정이다.

# Transfer Learning
Transfer Learning이란 특정 학습을 마친 모델을 다른 모델의 학습에 재사용 하는 기법이다. 모델 1을 학습할때의 경험을 모델 2를 학습할때 활용한다고 생각하면 편하다.
결국 우리는 어떤 문장의 분류(긍정, 부정을 판단하기 위함)가 궁극적인 목표이기 때문에 Transfer Learning이 중요하다고 볼 수 있다.(학습 데이터를 넣으면 이를 활용해 결과를 도출하는 모델을 만들자)

# Tokenization
NLP의 기초는 문장을 토큰화 하는것이다. 토큰화란 쉽게 이야기 하면 문장을 더 작은 단위로 나누는 방법이다.
한국어의 토큰화를 수행하는 대표적인 프로그램은 mecab,kkma가 있다. 단어 단위 토큰화, 문자 단위 토큰화, 서브워드 단위 토큰화 방법이 존재한다.
단어 단위는 단어(어절) 단위로 문장을 나누는것이기 때문에 간편하지만 표현이 살짝만 바뀌어도 다른 어휘로 인식한다.(먹었어, 먹었는데요를 다른 단어로 인식한다.)
문자 단위 토큰화는 단어가 아닌 문자 단위로 문장을 나누는 것이다. 이는 단어 단위 토큰화의 단점을 보완하지만 한국어에서는 문자 토큰 하나가 의미가 있는 단위가 되기는 어렵다.
서브단위 토큰화는 단어 단위와 문자 단위 토큰화의 중점에 있는 형태이다.(둘의 장점만을 취한 형태.)

# Byte Pair Encoding(BPE)
BPE는 정보 압축 알고리즘이다. 이를 통해 토큰화를 진행 할 수 있다. 예를 들어 aaabdaaabac는 aaa가 중복 되기 때문에 AbdAbac로 나타낼 수 있고 Ab가 여기서 또 겹치기 때문에 BdBac로 간단하게 나타 낼 수있다.
이 방식을 통해 문장을 간편하게 줄일 수 있다. 우리가 데이터를 표현하기 위한 사전 크기를 적절히 조절해 나가면서 정보를 효율적으로 압축할 수 있다.

```python
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

from Korpora import Korpora
nsmc = Korpora.load("nsmc", force_download=True)

import os
def write_lines(path, lines):
    with open(path, 'w', encoding='utf-8') as f:
        for line in lines:
            f.write(f'{line}\n')

write_lines("/content/train.txt", nsmc.train.get_all_texts())
write_lines("/content/test.txt", nsmc.test.get_all_texts())

import os
os.makedirs("/gdrive/My Drive/nlpbook/bbpe", exist_ok=True)

from tokenizers import ByteLevelBPETokenizer
bytebpe_tokenizer = ByteLevelBPETokenizer()
bytebpe_tokenizer.train(
    files=["/content/train.txt", "/content/test.txt"],
    vocab_size=10000,
    special_tokens=["[PAD]"]
)
bytebpe_tokenizer.save_model("/gdrive/My Drive/nlpbook/bbpe")


import os
os.makedirs("/gdrive/My Drive/nlpbook/wordpiece", exist_ok=True)

from tokenizers import BertWordPieceTokenizer
wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)
wordpiece_tokenizer.train(
    files=["/content/train.txt", "/content/test.txt"],
    vocab_size=10000,
)
wordpiece_tokenizer.save_model("/gdrive/My Drive/nlpbook/wordpiece")
```
구글 코랩을 이용한 토크나이저를 만들어본 코드이다.
```python
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

from transformers import GPT2Tokenizer
tokenizer_gpt = GPT2Tokenizer.from_pretrained("/gdrive/My Drive/nlpbook/bbpe")
tokenizer_gpt.pad_token = "[PAD]"


sentences = [
    "지금 시간은 11시",
    "오늘 저녁은 파스타!",
    "맛있었다..",
]
tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]

tokenized_sentences

sentences = [
    "지금 시간은 11시",
    "오늘 저녁은 파스타!",
    "맛있었다..",
]
batch_inputs = tokenizer_gpt(
    sentences,
    padding="max_length",
    max_length=12,
    truncation=True,
)
```
문장 3개를 활용해 토큰화를 해본 코드이다.


